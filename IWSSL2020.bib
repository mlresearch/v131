@Proceedings{IWSSL-2020,
  booktitle =	 {Proceedings of the First International Workshop on
                  Self-Supervised Learning},
  name =	 {International Workshop on Self-Supervised Learning},
  shortname =	 {IWSSL},
  editor =	 {Henry Minsky and Paul Robertson and Olivier
                  L. Georgeon and Milan Minsky and Cyrus Shaoul},
  volume =	 {1},
  year =	 {2020},
  start =	 {2020-02-27},
  end =		 {2020-02-28},
  published =	 {2020-09-01},
  url =		 {https://leela-ai.com/iwssl/index.html},
  address =	 {Cambridge, Massachusetts, USA},
  shortname =	 {iwssl}
}

@InProceedings{robertson20a,
  title =	 {IWSSL Introduction to this volume},
  author =	 {Robertson, Paul and Minsky, Henry and Shaoul, Cyrus
                  and Minsky, Milan and Georgeon, Olivier},
  pages =	 {1--4},
  abstract =	 {This collection of papers was presented at the first
                  annual international workshop on self- supervised
                  learning (IWSSL2020) held in Cambridge,
                  Massachusetts, between February 27 and February 28,
                  2020. They represent the state of the art in an
                  expanding field of research that attempts to build
                  systems that can learn without human intervention
                  with little or no hard-wired domain knowledge, as
                  would a new-born child or animal.}
}

@InProceedings{georgeon20,
  title =	 {Generating Natural Behaviors using Constructivist
                  Algorithms},
  author =	 {Georgeon, Olivier L. and Robertson, Paul and Xue,
                  Jianyong},
  pages =	 {5--14},
  abstract =	 {We present a project to design interactive devices
                  (smart displays, robots, etc.) capable of
                  self-motivated learning through non-goal-directed
                  interactive behaviors (e.g., curious, emotional,
                  playful behaviors). We use and improve algorithms
                  inspired by constructivist epistemology that we have
                  designed previously. These algorithms incrementally
                  learn se- quential hierarchies of control loops in a
                  bottom-up and open-ended fashion, and continu- ously
                  reuse the learned higher-level control loops to
                  generate increasingly complex behaviors that exhibit
                  self-motivation. This project contributes to
                  research in self-supervised learning because the
                  learning is driven by low-level preferences that
                  under-determine the device’s fu- ture behaviors,
                  leaving room for individuation, which, in turn,
                  opens the way to autonomy in learning.}
}

@InProceedings{robertson20b,
  title =	 {Continuous Learning of Action and State Spaces
                  (CLASS)},
  author =	 {Robertson, Paul and Georgeon, Olivier},
  pages =	 {15--31},
  abstract =	 {We present a novel approach to state space
                  discretization for constructivist and reinforcement
                  learning. Constructivist learning and reinforcement
                  learning often operate on a predefined set of states
                  and transitions (state space). AI researchers design
                  algorithms to reach particular goal states in this
                  state space (for example, visualized in the form of
                  goal cells that a robot should reach in a
                  grid). When the size and the dimensionality of the
                  state space increases, however, finding goal states
                  becomes intractable. It is nonetheless assumed that
                  these algorithms can have useful applications in the
                  physical world provided that there is a way to
                  construct a discrete state space of reasonable size
                  and dimensionality. Yet, the manner in which the
                  state space is discretized is the source of many
                  problems for both constructivist and reinforcement
                  learning approaches. The problems can roughly be
                  divided into two categories: (1) wiring too much
                  domain information into the solution, and (2)
                  requiring massive storage to represent the state
                  space (such as Q-tables. The problems relate to (1)
                  the non generality arising from wiring domain
                  information into the solution, and (2) non
                  scalability of the approach to useful domains
                  involving high dimensional state spaces. Another
                  important limitation is that high dimensional state
                  spaces require a massive number of learning
                  trials. We present a new approach that builds upon
                  ideas from place cells and cognitive maps.}
}

@InProceedings{thorisson20,
  title =	 {Seed-Programmed Autonomous General Learning},
  author =	 {Th\'orisson, Kristinn R.},
  pages =	 {32-70},
  abstract =	 {The knowledge that a natural learner creates of any
                  new situation will initially not only be partial but
                  very likely be partially incorrect. To improve
                  incomplete and incorrect knowl- edge with increased
                  experience – accumulated evidence – learning
                  processes must bring already-acquired knowledge
                  towards making sense of new situations. For the
                  initial creation of knowledge, and its subsequent
                  usage, expansion, modification, unification, and
                  deletion, knowledge construction mechanisms must be
                  self-guided, capable of self-supervised “sur- gical”
                  operation on existing knowledge, involving among
                  other things self-inspection or reflection. Further,
                  the information that makes up an agent’s knowledge
                  set must thus be structured in a way that supports
                  reflective processes including discrimination,
                  compari- son, and manipulation of arbitrary subsets
                  of the knowledge set. Few proposals for how to
                  achieve this in a parsimonious way exist. Here we
                  present a theory of how systems with these
                  properties may work, and how cumulative
                  self-supervised learning mechanisms can reach levels
                  of autonomy like those seen in individuals of many
                  animal species. Our theory rests on the hypotheses
                  that learning is (a) organized around causal
                  relations, (b) boot- strapped from observed
                  correlations, using (c) fine-grain relational
                  models, manipulated by (d) micro-ampliative
                  reasoning processes. We further hypothesize that a
                  machine properly constructed in this way will be (e)
                  capable of seed-programmed autonomous generality:
                  The ability to apply learning to any phenomenon –
                  that is, being domain-independent – provided that
                  (f) the seed reference observable variables at
                  “birth”, and that (g) new phenomena and existing
                  knowledge overlap on one or more observables or
                  inferred features. The theory is based on
                  implemented systems that have produced notable
                  results in the direction of increased general
                  machine intelligence.}
}

@InProceedings{lieberman20,
  title =	 {Intrinsic and Extrinsic Motivation in Intelligent
                  Systems},
  author =	 {Lieberman, Henry},
  pages =	 {71--79},
  abstract =	 {There are two ways that systems, human or machine,
                  can get ”motivated” to take action in problem
                  solving. One, they can be given goals by some
                  external entity. In some instances, they might have
                  no capability other than to work towards the goals
                  provided by that entity. Two, they can have their
                  own, internal goals, and work towards those
                  goals. If given a goal by an outside entity, they
                  can then try to figure out whether, and how, the
                  external goal might align with their internal
                  goals. In that case, the agent might be said to be
                  acting in a ”self-supervised” manner. There are, of
                  course, cases where both intrinsic and extrinsic
                  motivation come into play. This paper will argue
                  that many machine learning systems, as well as human
                  organiza- tions, put too much emphasis on extrinsic
                  motivation, and have not fully taken advantage of
                  the potential of intrinsic motivation. Reinforcement
                  learning systems, for example, have a ”reward
                  signal” that is the sole extrinsic motivating
                  factor. It is no wonder then, that even when such
                  systems work well, they are incapable of explaining
                  themselves, because they cannot express an
                  explanation in terms of their own (or their users’)
                  goals. In hu- man organizations, relying only on
                  extrinsic motivation (= ”incentive”) leads to rigid
                  or dictatorial organizations; engaging internal
                  motivation (at some cost to ”organizational
                  efficiency”) can lead to creativity and invention.}
}

@InProceedings{kommrusch20,
  title =	 {Self-Supervised Learning for Multi-Goal Grid World:
                  Comparing Leela and Deep Q Network},
  author =	 {Kommrusch, Steve},
  pages =	 {80--96},
  abstract =	 {Modern machine learning research has explored
                  numerous approaches to solving reinforce- ment
                  learning with multiple goals and sparse rewards as
                  well as learning correct actions from a small number
                  of exploratory samples. We explore the ability of a
                  self-supervised system which automatically creates
                  and tests symbolic hypotheses about the world to ad-
                  dress these same issues. Leela is a system which
                  builds an understanding of the world using
                  constructivist artificial intelligence. For our
                  study, we create an N ∗ N grid world with goals
                  related to proprioceptive or visual positions for
                  exploration. We compare Leela to a DQN which
                  includes hindsight for improving multigoal learning
                  with sparse rewards. Our results show that Leela is
                  able to learn to solve multigoal problems in an N ∗
                  N world with approximately 160N2 exploratory steps
                  compared to 360N2.7 steps required by the DQN.}
}

@InProceedings{macbeth20,
  title =	 {Primitive-Decomposed Cognitive Representations
                  Enhancing Learning with Primitive-Decomposed
                  Cognitive Representations},
  author =	 {Jamie C. Macbeth},
  pages =	 {97--106},
  abstract =	 {This paper proposes work that applies insights from
                  meaning representation systems for in-depth natural
                  language understanding to representations for
                  self-supervised learning systems, which show promise
                  in developing complex, deeply-nested symbolic
                  structures through self-motivated exploration of
                  their environments. The core of the representation
                  system transforms language inputs into language-free
                  structures that are complex combinations of
                  conceptual primitives, forming a substrate for
                  human-like understanding and common-sense
                  reasoning. We focus on decomposing representations
                  of expectation, intention, planning, and
                  decision-making which are essential to a
                  self-motivated learner. These meaning
                  representations may enhance learning by enabling a
                  rich array of mappings between new experiences and
                  structures stored in short-term and long-term
                  memory. We also argue that learning can be further
                  enhanced when language interaction itself is an
                  integral part of the environment in which the
                  self-supervised learning agent is embedded.}
}
